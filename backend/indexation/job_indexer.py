"""
============================================================================
SMARTHIRE - Job Indexer Module (FIXED)
Indexation automatique des offres d'emploi avec preprocessing NLP
============================================================================
"""

import json
import logging
import shutil
from pathlib import Path
from typing import Optional, Tuple

from whoosh.index import create_in, exists_in, open_dir
from whoosh.fields import Schema, TEXT, ID, KEYWORD, NUMERIC
from whoosh.writing import AsyncWriter

from backend.config.settings import JOB_FOLDER, JOB_INDEX, NIVEAU_MAPPING
from backend.extraction.skills_extractor import get_skills_database
from backend.indexation.preprocessing import (
    pretraiter_texte,
    pretraiter_competences
)

logger = logging.getLogger(__name__)

# ========================================================
# UTILITIES - Token Management
# ========================================================

def compter_tokens(text: str) -> int:
    """
    Compte le nombre de tokens dans un texte.
    Simple approximation: divise par espaces et ponctuation.
    
    Args:
        text: Texte √† analyser
        
    Returns:
        Nombre approximatif de tokens
    """
    if not text or not isinstance(text, str):
        return 0
    
    import re
    # Divise sur les espaces et ponctuation
    tokens = re.findall(r'\b\w+\b', text.lower())
    return len(tokens)


def calculer_reduction(tokens_original: int, tokens_processed: int) -> float:
    """
    Calcule le pourcentage de r√©duction de tokens apr√®s preprocessing.
    
    Args:
        tokens_original: Nombre de tokens avant preprocessing
        tokens_processed: Nombre de tokens apr√®s preprocessing
        
    Returns:
        Pourcentage de r√©duction (0.0 √† 100.0)
    """
    if tokens_original == 0:
        return 0.0
    
    reduction = ((tokens_original - tokens_processed) / tokens_original) * 100
    return max(0.0, min(100.0, reduction))  # Clamp entre 0 et 100


def extraire_localisation(job_json: dict) -> str:
    """
    Extrait la localisation de mani√®re robuste √† partir des donn√©es JSON.
    Essaie plusieurs chemins pour la trouver.
    
    Args:
        job_json: Dictionnaire JSON de l'offre
        
    Returns:
        Localisation extraite ou cha√Æne vide
    """
    # Chemin 1: localisation directe
    localisation = job_json.get("location", "").strip()
    if localisation:
        return localisation
    
    # Chemin 2: depuis l'objet company
    company = job_json.get("company", {})
    
    if isinstance(company, dict):
        # Essayer city d'abord
        city = company.get("city", "").strip()
        if city:
            # Ajouter le pays si disponible
            country = company.get("country", "").strip()
            return f"{city}, {country}" if country else city
        
        # Fallback: location depuis company
        loc = company.get("location", "").strip()
        if loc:
            return loc
    
    elif isinstance(company, str):
        # Si company est directement une cha√Æne, l'utiliser comme location
        return company.strip()
    
    return ""


def valider_job_id(job_id: any) -> Tuple[bool, str]:
    """
    Valide que job_id est pr√©sent et non vide.
    
    Args:
        job_id: L'ID √† valider
        
    Returns:
        Tuple (is_valid, error_message)
    """
    if not job_id:
        return False, "job_id manquant ou vide"
    
    job_id_str = str(job_id).strip()
    if not job_id_str:
        return False, "job_id ne peut pas √™tre vide apr√®s conversion en cha√Æne"
    
    return True, ""


# ========================================================
# SCH√âMA D'INDEXATION OFFRES
# ========================================================
job_schema = Schema(
    job_id=ID(stored=True, unique=True),
    titre_poste=TEXT(stored=True, field_boost=2.0),
    description=TEXT(stored=True),
    titre_poste_processed=TEXT(stored=True),
    description_processed=TEXT(stored=True),
    competences_requises=KEYWORD(commas=True, lowercase=True, stored=True, field_boost=1.5),
    localisation=TEXT(stored=True),
    niveau_souhaite=ID(stored=True),
    domaine=ID(stored=True),
    annees_min=NUMERIC(stored=True),
    annees_max=NUMERIC(stored=True),
    entreprise=TEXT(stored=True),
    type_contrat=TEXT(stored=True),
    mode_travail=TEXT(stored=True),
    
    # Statistiques NLP
    nb_tokens_original=NUMERIC(stored=True),
    nb_tokens_processed=NUMERIC(stored=True)
)

# ========================================================
# CLASSE D'INDEXATION
# ========================================================
class JobIndexer:
    """Classe pour indexer les offres d'emploi avec preprocessing NLP"""
    
    def __init__(self, job_folder: Path = JOB_FOLDER, index_dir: Path = JOB_INDEX):
        self.job_folder = job_folder
        self.index_dir = index_dir
        self.skills_db = get_skills_database()
        
        # Statistiques
        self.total_jobs = 0
        self.success_count = 0
        self.error_count = 0
    
    def _creer_index(self, force: bool = False):
        """Cr√©e ou recr√©e l'index"""
        if self.index_dir.exists() and force:
            shutil.rmtree(self.index_dir)
            logger.info(f"‚úÖ Ancien index supprim√©: {self.index_dir}")
        
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        if not exists_in(str(self.index_dir)):
            create_in(str(self.index_dir), job_schema)
            logger.info(f"‚úÖ Nouvel index cr√©√©: {self.index_dir}")
    
    def _charger_json(self, filepath: Path) -> Optional[dict]:
        """Charge un fichier JSON d'offre"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erreur JSON dans {filepath.name}: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture {filepath.name}: {e}")
            return None
    
    def _extraire_donnees_offre(self, job_json: dict) -> Optional[dict]:
        """
        Extrait et structure les donn√©es d'une offre avec preprocessing
        
        Returns:
            Dictionnaire avec toutes les donn√©es ou None en cas d'erreur
        """
        try:
            # 1Ô∏è‚É£ Extraction des champs de base
            job_id = job_json.get("job_id", "")
            titre_poste = job_json.get("title", "")
            
            # 2Ô∏è‚É£ Description compl√®te
            description_text = job_json.get("description", "")
            responsibilities = job_json.get("responsibilities", [])
            if responsibilities:
                description_text += " " + " ".join(responsibilities)
            
            # 3Ô∏è‚É£ Comp√©tences requises
            required_skills = job_json.get("required_skills", [])
            preferred_skills = job_json.get("preferred_skills", [])
            all_skills = required_skills + preferred_skills
            competences_list = [skill.lower() for skill in all_skills]
            competences_str = pretraiter_competences(competences_list)
            
            # 4Ô∏è‚É£ Pr√©traitement NLP
            skills_set = self.skills_db.get_skills_set()
            
            titre_processed, _ = pretraiter_texte(
                titre_poste,
                preserve_skills=True,
                skills_list=skills_set
            )
            
            description_processed, _ = pretraiter_texte(
                description_text,
                preserve_skills=True,
                skills_list=skills_set
            )
            
            # 5Ô∏è‚É£ Statistiques NLP - FIXED: utilise la fonction locale
            nb_tokens_original = compter_tokens(titre_poste + " " + description_text)
            nb_tokens_processed = compter_tokens(titre_processed + " " + description_processed)
            
            # 6Ô∏è‚É£ Autres champs - FIXED: utilise la fonction robuste d'extraction
            localisation = extraire_localisation(job_json)
            
            niveau_souhaite = job_json.get("experience_level", "Mid-Level")
            annees_min, annees_max = NIVEAU_MAPPING.get(niveau_souhaite, (0, 5))
            
            domaine = job_json.get("domain", "").lower()
            
            entreprise = ""
            company = job_json.get("company", {})
            if isinstance(company, dict):
                entreprise = company.get("name", "")
            elif isinstance(company, str):
                entreprise = company
            
            type_contrat = job_json.get("contract_type", "")
            mode_travail = job_json.get("work_mode", "")
            
            return {
                'job_id': job_id,
                'titre_poste': titre_poste,
                'description': description_text,
                'titre_poste_processed': titre_processed,
                'description_processed': description_processed,
                'competences_requises': competences_str,
                'competences_list': competences_list,
                'localisation': localisation,
                'niveau_souhaite': niveau_souhaite,
                'domaine': domaine,
                'annees_min': annees_min,
                'annees_max': annees_max,
                'entreprise': entreprise,
                'type_contrat': type_contrat,
                'mode_travail': mode_travail,
                'nb_tokens_original': nb_tokens_original,
                'nb_tokens_processed': nb_tokens_processed
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction donn√©es: {e}")
            return None
    
  
        # R√©cup√©ration des fichiers JSON
        try:
            json_files = sorted([
                f for f in self.job_folder.glob("*.json")
                if f.name != "all_jobs.json"  # Exclure le fichier agr√©g√©
            ])
            
            self.total_jobs = len(json_files)
            
            if self.total_jobs == 0:
                logger.warning(f"‚ö†Ô∏è Aucune offre trouv√©e dans {self.job_folder}")
                return
            
            logger.info(f"üìÅ {self.total_jobs} offres trouv√©es dans {self.job_folder}\n")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture dossier: {e}")
            return
        
        # Ouverture de l'index pour √©criture
        ix = open_dir(str(self.index_dir))
        writer = AsyncWriter(ix)
        
        # Traitement de chaque offre
        for i, filepath in enumerate(json_files, 1):
            try:
                # Chargement du JSON
                job_json = self._charger_json(filepath)
                
                if job_json is None:
                    self.error_count += 1
                    continue
                
                # Extraction des donn√©es
                job_data = self._extraire_donnees_offre(job_json)
                
                if job_data is None:
                    self.error_count += 1
                    continue
                
                # Indexation
                writer.add_document(
                    job_id=job_data['job_id'],
                    titre_poste=job_data['titre_poste'],
                    description=job_data['description'],
                    titre_poste_processed=job_data['titre_poste_processed'],
                    description_processed=job_data['description_processed'],
                    competences_requises=job_data['competences_requises'],
                    localisation=job_data['localisation'],
                    niveau_souhaite=job_data['niveau_souhaite'],
                    domaine=job_data['domaine'],
                    annees_min=job_data['annees_min'],
                    annees_max=job_data['annees_max'],
                    entreprise=job_data['entreprise'],
                    type_contrat=job_data['type_contrat'],
                    mode_travail=job_data['mode_travail'],
                    nb_tokens_original=job_data['nb_tokens_original'],
                    nb_tokens_processed=job_data['nb_tokens_processed']
                )
                
                # Affichage du r√©sum√©
                self._afficher_resume_offre(i, job_data)
                
                self.success_count += 1
                
            except Exception as e:
                logger.error(f"{i:02d}/{self.total_jobs} ‚Üí ‚ùå ERREUR: {filepath.name} - {e}")
                self.error_count += 1
                continue
        
        # Commit des changements
        try:
            writer.commit()
            self._afficher_statistiques_finales()
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du commit: {e}")
    
    def _afficher_resume_offre(self, index: int, job_data: dict):
        """Affiche un r√©sum√© format√© de l'offre index√©e"""
        # Preview des comp√©tences
        skills_list = job_data['competences_list']
        skills_preview = ", ".join(skills_list[:5])
        if len(skills_list) > 5:
            skills_preview += f" (+ {len(skills_list) - 5} autres)"
        
        # R√©duction tokens
        reduction = calculer_reduction(
            job_data['nb_tokens_original'],
            job_data['nb_tokens_processed']
        )
        
        # Preview du titre processed
        titre_preview = job_data['titre_poste_processed'][:80]
        if len(job_data['titre_poste_processed']) > 80:
            titre_preview += "..."
        
        print(f"\n{index:02d}/{self.total_jobs} {'='*100}")
        print(f"  üÜî JOB ID:           {job_data['job_id']}")
        print(f"  üíº TITRE:            {job_data['titre_poste']}")
        print(f"  üè¢ ENTREPRISE:       {job_data['entreprise']}")
        print(f"  üìç LOCALISATION:     {job_data['localisation']}")
        print(f"  üìä NIVEAU:           {job_data['niveau_souhaite']} ({job_data['annees_min']}-{job_data['annees_max']} ans)")
        print(f"  üè∑Ô∏è  DOMAINE:          {job_data['domaine'].upper()}")
        print(f"  üõ†Ô∏è  COMP√âTENCES:      {len(skills_list)} skills ‚Üí {skills_preview}")
        print(f"  üî§ TOKENS NLP:       {job_data['nb_tokens_original']} ‚Üí {job_data['nb_tokens_processed']} (-{reduction:.1f}%)")
        print(f"  üìù PREVIEW:          {titre_preview}")
    
    def _afficher_statistiques_finales(self):
        """Affiche les statistiques finales de l'indexation"""
        logger.info("\n" + "="*120)
        logger.info("‚úÖ INDEXATION AUTOMATIQUE TERMIN√âE AVEC SUCC√àS")
        logger.info("="*120)
        logger.info(f"\nüìä R√©sum√©:")
        logger.info(f"   ‚Ä¢ Offres index√©es avec succ√®s: {self.success_count}")
        logger.info(f"   ‚Ä¢ Offres en erreur: {self.error_count}")
        logger.info(f"   ‚Ä¢ Total trait√©: {self.total_jobs}")
        logger.info(f"   ‚Ä¢ Taux de succ√®s: {(self.success_count/self.total_jobs*100):.1f}%")
        logger.info(f"\nüìÅ Index sauvegard√©: {self.index_dir}")
        logger.info(f"\nüîç Pipeline NLP appliqu√©:")
        logger.info(f"   ‚úì Chargement JSON")
        logger.info(f"   ‚úì Minuscules")
        logger.info(f"   ‚úì Suppression ponctuation")
        logger.info(f"   ‚úì Tokenisation (NLTK)")
        logger.info(f"   ‚úì Suppression stopwords (EN + FR)")
        logger.info(f"   ‚úì Lemmatisation (WordNet)")
        logger.info(f"   ‚úì Pr√©servation des comp√©tences techniques")


# ========================================================
# FONCTION PRINCIPALE
# ========================================================
def indexer_offres_automatique(force: bool = False):
    """
    Point d'entr√©e principal pour l'indexation automatique
    
    Args:
        force: Si True, recr√©e l'index compl√®tement
    """
    indexer = JobIndexer()
    indexer.indexer_toutes_les_offres(force=force)


# ========================================================
# FONCTION D'INDEXATION EN TEMPS R√âEL (FIXED)
# ========================================================
def indexer_offre_depuis_donnees(
    job_id: str,
    job_data: dict,
    user_id: str = ""
) -> bool:
    """
    Indexe une offre d'emploi en temps r√©el apr√®s soumission par le recruteur.
    
    FIXED:
    - Validation robuste de job_id
    - Gestion des doublons (suppression avant ajout)
    - Gestion d'erreurs compl√®te
    
    Args:
        job_id: L'ID de l'offre dans la base de donn√©es PostgreSQL.
        job_data: Dictionnaire contenant les donn√©es brutes de l'offre.
        user_id: ID du recruteur qui a post√© l'offre.
        
    Returns:
        True si l'indexation r√©ussit, False sinon.
    """
    # FIXED: Validation de job_id
    is_valid, error_msg = valider_job_id(job_id)
    if not is_valid:
        logger.error(f"‚ùå Validation √©chou√©e pour l'offre: {error_msg}")
        return False
    
    job_id_str = str(job_id).strip()
    
    try:
        # Initialisation de la classe pour acc√©der √† la logique d'extraction et de preprocessing
        indexer = JobIndexer() 
        
        # Utilisation de la m√©thode interne pour structurer et pr√©traiter les donn√©es
        job_data_processed = indexer._extraire_donnees_offre(job_data)
        
        if job_data_processed is None:
            logger.error(f"‚ùå √âchec du pr√©traitement pour l'offre #{job_id_str}.")
            return False

        # Mise √† jour de l'ID
        job_data_processed['job_id'] = job_id_str

        # 1. Ouverture de l'index Whoosh
        ix = open_dir(str(JOB_INDEX))
        
        # FIXED: Suppression du doublon si existant
        try:
            with ix.searcher() as searcher:
                from whoosh.qparser import QueryParser
                query_parser = QueryParser("job_id", ix.schema)
                query = query_parser.parse(job_id_str)
                results = searcher.search(query)
                
                if len(results) > 0:
                    logger.info(f"‚ö†Ô∏è Offre #{job_id_str} existe d√©j√†. Suppression avant r√©indexation.")
                    
                    # Suppression via AsyncWriter
                    with ix.writer() as writer:
                        writer.delete_by_term("job_id", job_id_str)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è V√©rification doublon √©chou√©e: {e}. Continuant l'indexation...")

        # 2. Indexation du document (nouveau writer apr√®s suppression)
        writer = AsyncWriter(ix)
        
        try:
            writer.add_document(
                job_id=job_data_processed['job_id'],
                titre_poste=job_data_processed['titre_poste'],
                description=job_data_processed['description'],
                titre_poste_processed=job_data_processed['titre_poste_processed'],
                description_processed=job_data_processed['description_processed'],
                competences_requises=job_data_processed['competences_requises'],
                localisation=job_data_processed['localisation'],
                niveau_souhaite=job_data_processed['niveau_souhaite'],
                domaine=job_data_processed['domaine'],
                annees_min=job_data_processed['annees_min'],
                annees_max=job_data_processed['annees_max'],
                entreprise=job_data_processed['entreprise'],
                type_contrat=job_data_processed['type_contrat'],
                mode_travail=job_data_processed['mode_travail'],
                nb_tokens_original=job_data_processed['nb_tokens_original'],
                nb_tokens_processed=job_data_processed['nb_tokens_processed']
            )
            
            # 3. Commit
            writer.commit()
            logger.info(f"‚úÖ Offre d'emploi #{job_id_str} index√©e en temps r√©el par le recruteur {user_id}.")
            return True
        
        except Exception as e:
            writer.cancel()
            logger.error(f"‚ùå Erreur lors de l'ajout du document #{job_id_str}: {e}")
            return False
    
    except Exception as e:
        logger.error(f"‚ùå √âchec indexation offre #{job_id_str}: {e}")
        return False


if __name__ == "__main__":
    import sys
    from backend.utils.logger import setup_logging
    
    # Configuration du logging
    setup_logging()
    
    # Indexation automatique
    force_recreate = "--force" in sys.argv
    
    if force_recreate:
        logger.info("üîÑ Mode FORCE: L'index sera compl√®tement recr√©√©")
    
    indexer_offres_automatique(force=force_recreate)